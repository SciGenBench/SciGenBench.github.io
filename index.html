<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Scientific Image Synthesis, Benchmarking, Methodologies, Downstream Utility">
  <!-- TODO: List all authors -->
  <meta name="author" content="Anonymous Authors">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://scigenbench.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://scigenbench.github.io/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility - Research Preview">
  <meta property="article:published_time" content="2026-01-11T00:00:00.000Z">
  <meta property="article:author" content="Anonymous Authors">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Scientific Image Synthesis">
  <meta property="article:tag" content="Benchmarking">
  <meta property="article:tag" content="Methodologies">
  <meta property="article:tag" content="Downstream Utility">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility">
  <meta name="citation_author" content="Anonymous Authors">
  <meta name="citation_publication_date" content="2026-01-11">
  <meta name="citation_conference_title" content="Under Review">
  <meta name="citation_pdf_url" content="change to arxiv paper url">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>SciGenBench</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://scigenbench.github.io/" target="_blank">Anonymous Authors</a><sup>*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">Under Review</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <!-- <div class="column has-text-centered"> -->
                    <!-- <div class="publication-links"> -->
                         <!-- TODO: Update with your arXiv paper ID -->
                      <!-- <span class="link-block"> -->
                        <!-- <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" -->
                        <!-- class="external-link button is-normal is-rounded is-dark"> -->
                        <!-- <span class="icon"> -->
                          <!-- <i class="fas fa-file-pdf"></i> -->
                        <!-- </span> -->
                        <!-- <span>Paper</span> -->
                      <!-- </a> -->
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="https://hub.zenoml.com/project/b468f508-6492-40f2-8ff3-9db8db44c1b7/SciGenBench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Model Outputs</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/scigenbench/scigenbench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual–logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit “understand → plan → code” workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness–precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method</h2>
          <figure style="width: 100%; margin: 0 auto 1.5rem auto; display: block;">
            <img src="static/images/method.png" alt="ImgCoder: Understand → Plan → Code" style="width: 100%; height: auto; display: block; margin: 0 auto;">
            <figcaption style="text-align: center !important; margin-top: 0.75rem; font-size: 0.9em; color: #4a4a4a; line-height: 1.6; width: 100%; display: block; padding: 0;">
              <b>Methodological Overview.</b> 
              The framework consists of three core components: 
              (1) <b>Scientific Image Generation</b> (Left), where we propose ImgCoder, a programmatic approach decoupling planning from implementation to outperform pixel-based baselines; 
              (2) <b>SciGenBench Construction</b> (Top Right), a rigorously curated benchmark with a fine-grained taxonomy and atomic quizzes; and 
              (3) <b>Evaluation Framework</b> (Bottom Right), a multi-faceted assessment system combining LLM judges, inverse validation, standard metrics, and downstream performance.
            </figcaption>
          </figure>
          <div class="level-set has-text-justified">
            <p>
              We study scientific image synthesis under two paradigms: <b>pixel-based generation</b> and <b>programmatic synthesis</b>.
              To improve structural correctness in diagram-heavy scientific domains, we propose <b>ImgCoder</b>, a logic-driven framework that
              follows an explicit <b>Understand → Plan → Code</b> workflow. Concretely, ImgCoder first parses the problem to extract scientific entities
              and constraints, then produces an explicit layout and labeling plan, and finally emits executable rendering code (e.g., Python/Matplotlib/TikZ)
              to deterministically generate the figure. This decoupling of reasoning from rendering improves precision and reduces structure-level hallucinations.
            </p>
        
            <p>
              We further introduce <b>SciGenBench</b> to evaluate scientific image synthesis along two axes:
              <b>Information Utility</b> via Inverse Quiz Validation (<b>R<sub>inv</sub></b>) and <b>Logical Correctness</b> via LMM-as-Judge scores.
              Together, the benchmark and methodology enable systematic analysis of generation failures and support downstream training with rigorously verified synthetic images.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">SciGenBench Leaderboard</h2>
          <!-- <center> -->
          <!-- <img src="static/images/architecture_finetune.png" alt="Mixed Video-Image Finetuning" class="center-image blend-img-background"/> -->
          <!-- </center> -->
          <div class="level-set has-text-justified">
            <p>
              <b>SciGenBench</b> evaluates scientific image generation on two core dimensions:
              <b>Information Utility</b> via Inverse Quiz Validation (<b>R<sub>inv</sub></b>, ↑) and
              <b>Logical Correctness</b> via LMM-as-Judge scores (0–2, ↑). Standard metrics are computed on the real-image SeePhys subset
              (PSNR ↑, SSIM ↑, CLIP ↑, FID ↓).
            </p>
          </div>

          <div class="table-container" style="margin-top: 1rem;">
            <style>
              .table-container table tbody td:first-child,
              .table-container table thead th:first-child {
                text-align: left !important;
              }
            </style>
            <table class="table is-striped is-hoverable is-fullwidth is-size-8">
              <thead>
                <tr>
                  <th style="text-align: left;">Model</th>
                  <th>R<sub>inv</sub> ↑</th>
                  <th>C&amp;F</th>
                  <th>L&amp;P</th>
                  <th>R&amp;O</th>
                  <th>SP</th>
                  <th>E&amp;R</th>
                  <th>PSNR</th>
                  <th>SSIM</th>
                  <th>CLIP</th>
                  <th>FID ↓</th>
                </tr>
              </thead>
        
              <tbody>
                <tr><td colspan="11"><b>Open-source T2I Models</b></td></tr>
                <tr>
                  <td>HunyuanImage-3.0</td><td>30.79</td><td>0.39</td><td>0.78</td><td>1.44</td><td>0.56</td><td>0.81</td><td>12.21</td><td>0.82</td><td>25.01</td><td>93.27</td>
                </tr>
                <tr>
                  <td>Qwen-Image</td><td>38.86</td><td>0.24</td><td>0.70</td><td>1.48</td><td>0.30</td><td>0.76</td><td>9.63</td><td>0.78</td><td>25.02</td><td>120.42</td>
                </tr>
        
                <tr><td colspan="11"><b>Closed-source T2I Models</b></td></tr>
                <tr>
                  <td>GPT-Image-1</td><td>42.97</td><td>0.57</td><td>1.37</td><td>1.90</td><td>0.84</td><td>1.19</td><td>13.07</td><td>0.84</td><td>25.14</td><td><b>77.31</b></td>
                </tr>
                <tr>
                  <td>Seedream-4.0</td><td>52.67</td><td>0.44</td><td>0.94</td><td>1.67</td><td>0.55</td><td>0.95</td><td>10.65</td><td>0.74</td><td>25.02</td><td>98.22</td>
                </tr>
                <tr>
                  <td>Nanobanana</td><td>57.75</td><td>0.43</td><td>0.92</td><td>1.60</td><td>0.60</td><td>1.15</td><td>14.12</td><td>0.85</td><td>25.13</td><td>104.70</td>
                </tr>
                <tr>
                  <td>Flux2-Flex</td><td>58.83</td><td>0.48</td><td>1.06</td><td>1.70</td><td>0.67</td><td>1.20</td><td>14.11</td><td>0.85</td><td>25.10</td><td>96.74</td>
                </tr>
                <tr>
                  <td>GPT-Image-1.5</td><td>63.52</td><td>0.98</td><td>1.70</td><td><b>1.97</b></td><td>1.17</td><td>1.62</td><td><b>14.79</b></td><td><b>0.88</b></td><td>25.16</td><td>112.52</td>
                </tr>
                <tr>
                  <td>Nanobanana-Pro</td><td><b>73.41</b></td><td>1.59</td><td>1.87</td><td><b>1.98</b></td><td>1.72</td><td><b>1.93</b></td><td>12.02</td><td>0.81</td><td>25.01</td><td>87.72</td>
                </tr>
        
                <tr><td colspan="11"><b>ImgCoder</b></td></tr>
                <tr>
                  <td>Qwen3-ImgCoder</td><td>56.38</td><td>1.21</td><td>1.30</td><td>1.62</td><td>1.39</td><td>1.29</td><td>14.71</td><td>0.86</td><td><b>25.21</b></td><td>121.55</td>
                </tr>
                <tr>
                  <td>Gemini-3-Flash-ImgCoder</td><td>76.93</td><td>1.80</td><td>1.88</td><td>1.88</td><td>1.92</td><td>1.91</td><td>14.63</td><td>0.85</td><td>25.18</td><td>117.83</td>
                </tr>
                <tr>
                  <td>Gemini-3-Pro-ImgCoder</td><td><b>77.87</b></td><td><b>1.82</b></td><td><b>1.93</b></td><td>1.91</td><td><b>1.93</b></td><td>1.90</td><td>14.59</td><td>0.86</td><td>25.16</td><td>107.67</td>
                </tr>
              </tbody>
            </table>
          </div>
        
          <div class="level-set has-text-justified" style="margin-top: 0.75rem;">
            <p class="is-size-8">
              <b>Judge dimensions:</b> C&amp;F = Correctness &amp; Fidelity, L&amp;P = Layout &amp; Precision, R&amp;O = Readability &amp; Occlusion,
              SP = Scientific Plausibility, E&amp;R = Expressiveness &amp; Richness.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{anonymous2025scientific,
  title={Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility},
  author={Anonymous Authors},
  journal={Under Review},
  year={2025},
  url={https://scigenbench.github.io/}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
